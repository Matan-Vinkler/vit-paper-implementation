{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "ntomsMkgw6tM",
        "MCZsjq8YsD_7",
        "cZzKglKCrJvu",
        "BpFyUIO8rT88",
        "QXzWp1VJr42L"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matan-Vinkler/vit-paper-implementation/blob/main/vit_implementation_tiny.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vision Transformer (Vit) - From-Scratch Implementation"
      ],
      "metadata": {
        "id": "6lseutAFvpMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains a **from-scratch reimplementation** of the paper  \n",
        "[*An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale*](https://arxiv.org/pdf/2010.11929) (Dosovitskiy et al., 2020).\n",
        "\n",
        "**🎯 Project Goal**\n",
        "- Read and understand a research paper,\n",
        "- Translate the architecture into working **PyTorch code** without relying on prebuilt models,\n",
        "- Validate correctness with unit tests, overfit-one-batch experiments, and CIFAR-10 training.\n",
        "\n",
        "**🏗️ Components Implemented**\n",
        "- Patchify / Unpatchify\n",
        "- Patch projection + CLS token + learnable positional embeddings\n",
        "- Multi-Head Self Attention (MSA)\n",
        "- MLP feedforward block\n",
        "- Transformer Encoder Block (Pre-LN, residuals, dropout)\n",
        "- Vision Transformer backbone (stacked encoder blocks)\n",
        "- Classification head (Linear or MLP)\n",
        "\n",
        "**🧪 Validation**\n",
        "- Gradient flow and shape checks\n",
        "- CIFAR-10 training with AdamW, warmup+cosine LR, AMP\n",
        "\n",
        "**📊 Results**\n",
        "- On CIFAR-10 resized to 224×224: ~58–60% validation accuracy after 30 epochs with a small ViT.\n",
        "- Performance is **consistent with the paper’s claim**: ViTs require large-scale data (e.g. ImageNet-21k, JFT) to truly outperform CNNs.\n"
      ],
      "metadata": {
        "id": "mITR0A0vwAe6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Table of Content"
      ],
      "metadata": {
        "id": "ntomsMkgw6tM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Setup and Imports](#scrollTo=MCZsjq8YsD_7)\n",
        "\n",
        ">[Define Model Architecture and Algorithms](#scrollTo=cZzKglKCrJvu)\n",
        "\n",
        ">[Data Load and Preprocessing](#scrollTo=BpFyUIO8rT88)\n",
        "\n",
        ">[Training Loop and Metrics](#scrollTo=QXzWp1VJr42L)"
      ],
      "metadata": {
        "id": "xW_W6cwEwkTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup and Imports"
      ],
      "metadata": {
        "id": "MCZsjq8YsD_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing neccesary libraries:"
      ],
      "metadata": {
        "id": "shIAyDlYsJzj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0S7q9N5oYpet"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "from typing import Tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting random seed and checking for available CUDA device:"
      ],
      "metadata": {
        "id": "CpmBSx61uEez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed=1337):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "set_seed(1337)"
      ],
      "metadata": {
        "id": "B0XJ3HpHuDBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Model Architecture and Algorithms"
      ],
      "metadata": {
        "id": "cZzKglKCrJvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create out `patchify` and `unpatchify` function.\n",
        "\n",
        "- `patchify` - split images into non-overlapping patches and flatten them.\n",
        "- `unpatchify` - reconstruct images from flattened patches."
      ],
      "metadata": {
        "id": "OWQUNKO0sQoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def patchify(x: torch.Tensor, patch_size: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Split images into non-overlapping patches and flatten them.\n",
        "\n",
        "    Args:\n",
        "        x: Tensor [B, C, H, W]\n",
        "        patch_size: P, must divide H and W exactly.\n",
        "\n",
        "    Returns:\n",
        "        Tensor [B, N, P*P*C], where N = (H//P) * (W//P)\n",
        "    \"\"\"\n",
        "    if x.dim() != 4:\n",
        "        raise ValueError(f\"Expected 4D tensor [B, C, H, W], got shape {tuple(x.shape)}\")\n",
        "\n",
        "    B, C, H, W = x.shape\n",
        "    P = patch_size\n",
        "\n",
        "    if (H % P) != 0 or (W % P) != 0:\n",
        "        raise ValueError(f\"patch_size={P} must divide H={H} and W={W} exactly.\")\n",
        "\n",
        "    h = H // P\n",
        "    w = W // P\n",
        "\n",
        "    x = x.reshape(B, C, h, P, w, P)\n",
        "    x = x.permute(0, 2, 4, 3, 5, 1)\n",
        "    x = x.reshape(B, h * w, P * P * C)\n",
        "\n",
        "    return x\n",
        "\n",
        "def unpatchify(patches: torch.Tensor, patch_size: int, img_size: Tuple[int, int], channels: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Reconstruct images from flattened patches.\n",
        "\n",
        "    Args:\n",
        "        patches: Tensor [B, N, P*P*C]\n",
        "        patch_size: P used in patchify\n",
        "        img_size: (H, W) of the original image\n",
        "        channels: C\n",
        "\n",
        "    Returns:\n",
        "        Tensor [B, C, H, W]\n",
        "    \"\"\"\n",
        "    if patches.dim() != 3:\n",
        "        raise ValueError(f\"Expected 3D tensor [B, N, P*P*C], got shape {tuple(patches.shape)}\")\n",
        "    B, N, flat = patches.shape\n",
        "    P = patch_size\n",
        "    H, W = img_size\n",
        "    C = channels\n",
        "    if flat != P * P * C:\n",
        "        raise ValueError(f\"Last dim {flat} != P*P*C = {P*P*C}\")\n",
        "    if (H % P) != 0 or (W % P) != 0:\n",
        "        raise ValueError(f\"patch_size={P} must divide H={H} and W={W} exactly.\")\n",
        "    h = H // P\n",
        "    w = W // P\n",
        "    if N != h * w:\n",
        "        raise ValueError(f\"Num patches N={N} != (H//P)*(W//P) = {h*w}\")\n",
        "    # [B, N, P*P*C] -> [B, h, w, P, P, C]\n",
        "    x = patches.reshape(B, h, w, P, P, C)\n",
        "    # -> [B, C, h, P, w, P]\n",
        "    x = x.permute(0, 5, 1, 3, 2, 4)\n",
        "    # -> [B, C, H, W]\n",
        "    x = x.reshape(B, C, H, W)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ls-NR88saNUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now create the linear projection module:"
      ],
      "metadata": {
        "id": "SiBtXhdZsnNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchProjection(nn.Module):\n",
        "    \"\"\"\n",
        "    Linear projection of flattened patches to D-dim embeddings.\n",
        "    Input:  [B, C, H, W]\n",
        "    Output: [B, N, D]\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size: int = 224, patch_size: int = 16,\n",
        "                 in_chans: int = 3, embed_dim: int = 192):\n",
        "        super().__init__()\n",
        "        if img_size % patch_size != 0:\n",
        "            raise ValueError(\"img_size must be divisible by patch_size.\")\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.in_chans = in_chans\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Linear(patch_size * patch_size * in_chans, embed_dim)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.proj.weight)\n",
        "        if self.proj.bias is not None:\n",
        "            nn.init.zeros_(self.proj.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, C, H, W = x.shape\n",
        "        if C != self.in_chans:\n",
        "            raise ValueError(f\"in_chans={self.in_chans} but got C={C}\")\n",
        "        if H != self.img_size or W != self.img_size:\n",
        "            raise ValueError(f\"Expected H=W={self.img_size}, got ({H},{W})\")\n",
        "        patches = patchify(x, self.patch_size)   # [B, N, P*P*C]\n",
        "        z = self.proj(patches)                   # [B, N, D]\n",
        "        return z"
      ],
      "metadata": {
        "id": "r4xOlI9Ggdls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This module inserts `[CLS]` token at the start of the embedding patches:"
      ],
      "metadata": {
        "id": "f1d1Xwq7sx7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AddCLSToken(nn.Module):\n",
        "    \"\"\"\n",
        "    Adds a learnable [CLS] token to the beginning of the sequence.\n",
        "    Input:  [B, N, D]\n",
        "    Output: [B, N+1, D]\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim: int):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        nn.init.normal_(self.cls_token, std=0.02)\n",
        "\n",
        "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        B, N, D = z.shape\n",
        "        if D != self.embed_dim:\n",
        "            raise ValueError(f\"Expected embed_dim={self.embed_dim}, got {D}\")\n",
        "        cls = self.cls_token.expand(B, -1, -1)   # [B, 1, D]\n",
        "        return torch.cat((cls, z), dim=1)        # [B, N+1, D]"
      ],
      "metadata": {
        "id": "TWxhrBPTDX76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This module adds positional embedding for the patches:"
      ],
      "metadata": {
        "id": "4SgtZi7ss93T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AddPositionalEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Adds a learnable positional embedding (including index 0 for CLS).\n",
        "    Input:  [B, N+1, D]\n",
        "    Output: [B, N+1, D]\n",
        "    \"\"\"\n",
        "    def __init__(self, num_patches: int, embed_dim: int):\n",
        "        super().__init__()\n",
        "        self.num_tokens = num_patches + 1     # account for CLS\n",
        "        self.embed_dim = embed_dim\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_tokens, embed_dim))\n",
        "        nn.init.normal_(self.pos_embed, std=0.02)\n",
        "\n",
        "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        B, T, D = z.shape\n",
        "        if T != self.num_tokens or D != self.embed_dim:\n",
        "            raise ValueError(f\"Expected [B, {self.num_tokens}, {self.embed_dim}], got {tuple(z.shape)}\")\n",
        "        return z + self.pos_embed              # broadcast add"
      ],
      "metadata": {
        "id": "N-4Uk7GVDa9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine `PatchProjection`, `AddCLSToken` and `AddPositionalEmbedding` into full tokenizer:"
      ],
      "metadata": {
        "id": "cEn8tDmTtEyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTTokenizer(nn.Module):\n",
        "    \"\"\"\n",
        "    Convenience wrapper: image -> patch projection -> add CLS -> add pos.\n",
        "    Input:  [B, C, H, W]\n",
        "    Output: [B, N+1, D]\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size: int = 224, patch_size: int = 16, in_chans: int = 3, embed_dim: int = 192):\n",
        "        super().__init__()\n",
        "        self.patch_proj = PatchProjection(img_size, patch_size, in_chans, embed_dim)\n",
        "        self.add_cls = AddCLSToken(embed_dim)\n",
        "        self.add_pos = AddPositionalEmbedding(self.patch_proj.num_patches, embed_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        z = self.patch_proj(x)   # [B, N, D]\n",
        "        z = self.add_cls(z)      # [B, N+1, D]\n",
        "        z = self.add_pos(z)      # [B, N+1, D]\n",
        "        return z"
      ],
      "metadata": {
        "id": "g5nibm9HDqpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define our multihead self-attention model:"
      ],
      "metadata": {
        "id": "Dc22G_pKtRuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multihead self-attention.\n",
        "    Input:  [B, T, D]\n",
        "    Output: [B, T, D]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim: int, num_heads: int, attn_dropout_value: float = 0.0, proj_dropout_value: float = 0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        if embed_dim % num_heads != 0:\n",
        "           raise ValueError(\"embed_dim must be divisible by num_heads\")\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.w_q = nn.Linear(embed_dim, embed_dim)\n",
        "        self.w_k = nn.Linear(embed_dim, embed_dim)\n",
        "        self.w_v = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        self.attn_drop = nn.Dropout(attn_dropout_value)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.proj_drop = nn.Dropout(proj_dropout_value)\n",
        "\n",
        "        # Init (xavier init + zero bias)\n",
        "        for m in [self.w_q, self.w_k, self.w_v, self.proj]:\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, z: torch.Tensor, return_attn: bool = False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [B, T, D]\n",
        "            return_attn: if True, also returns attention weights [B, h, T, T]\n",
        "        Returns:\n",
        "            out: [B, T, D] (and optionally attn)\n",
        "        \"\"\"\n",
        "        B, T, D = z.shape\n",
        "\n",
        "        if(D != self.embed_dim):\n",
        "            raise ValueError(f\"Expected embed_dim={self.embed_dim}, got {D}\")\n",
        "\n",
        "        # Apply linear projection\n",
        "        q = self.w_q(z)   # [B, T, D]\n",
        "        k = self.w_k(z)   # [B, T, D]\n",
        "        v = self.w_v(z)   # [B, T, D]\n",
        "\n",
        "        # Split into heads\n",
        "        def split_heads(t):\n",
        "            return t.view(B, T, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        q = split_heads(q)   # [B, h, T, head_dim]\n",
        "        k = split_heads(k)   # [B, h, T, head_dim]\n",
        "        v = split_heads(v)   # [B, h, T, head_dim]\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale    # [B, h, T, T]\n",
        "        attn = scores.softmax(dim=-1)                                 # [B, h, T, T]\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        # Apply attention to values\n",
        "        z = torch.matmul(attn, v)                                     # [B, h, T, head_dim]\n",
        "        z = z.permute(0, 2, 1, 3).reshape(B, T, D)                    # [B, T, D]\n",
        "        z = self.proj(z)                                              # [B, T, D]\n",
        "        z = self.proj_drop(z)\n",
        "\n",
        "        if return_attn:\n",
        "            return z, attn\n",
        "        else:\n",
        "            return z"
      ],
      "metadata": {
        "id": "H5X2cZd_W4ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define out multi-layer perceptron:"
      ],
      "metadata": {
        "id": "joVo-ybqtWZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-layer perceptron: Linear -> GELU -> Dropout -> Linear -> Dropout\n",
        "    Input:  [B, T, D]\n",
        "    Output: [B, T, D]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim: int, mlp_hidden_mult: int = 4, dropout_value: float = 0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        hidden = embed_dim * mlp_hidden_mult\n",
        "        self.w1 = nn.Linear(embed_dim, hidden)\n",
        "        self.act = nn.GELU()\n",
        "        self.drop1 = nn.Dropout(dropout_value)\n",
        "        self.w2 = nn.Linear(hidden, embed_dim)\n",
        "        self.drop2 = nn.Dropout(dropout_value)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.w1.weight)\n",
        "        nn.init.zeros_(self.w1.bias)\n",
        "        nn.init.xavier_uniform_(self.w2.weight)\n",
        "        nn.init.zeros_(self.w2.bias)\n",
        "\n",
        "    def forward(self, z: torch.Tensor):\n",
        "        z = self.w1(z)\n",
        "        z = self.act(z)\n",
        "        z = self.drop1(z)\n",
        "        z = self.w2(z)\n",
        "        z = self.drop2(z)\n",
        "        return z"
      ],
      "metadata": {
        "id": "0ZxkwNWpei1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine `MultiHeadSelfAttention` and `MLP` into `TransformerEncoderBlock`:"
      ],
      "metadata": {
        "id": "mnNHaemMteS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer Encoder block:\n",
        "    y = x + MSA(LN(x))\n",
        "    z = y + MLP(LN(y))\n",
        "    Input:  [B, T, D]\n",
        "    Output: [B, T, D]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim: int, num_heads: int, mlp_hidden_mult: int = 4, attn_dropout_value: float = 0.0, proj_dropout_value: float = 0.0, mlp_dropout_value: float = 1.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.msa = MultiHeadSelfAttention(embed_dim, num_heads, attn_dropout_value, proj_dropout_value)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = MLP(embed_dim, mlp_hidden_mult, mlp_dropout_value)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() != 3:\n",
        "            raise ValueError(f\"Expected 3D tensor [B, T, D], got shape {tuple(x.shape)}\")\n",
        "        B, T, D = x.shape\n",
        "        if D != self.embed_dim:\n",
        "            raise ValueError(f\"Expected embed_dim={self.embed_dim}, got {D}\")\n",
        "\n",
        "        z = x + self.msa(self.norm1(x))\n",
        "        z = z + self.mlp(self.norm2(z))\n",
        "        return z"
      ],
      "metadata": {
        "id": "lRj0LV4eksx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create classification head - linear:"
      ],
      "metadata": {
        "id": "juEVh7UTtgPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationHeadLinear(nn.Module):\n",
        "    \"\"\"\n",
        "    Classification linear head layer\n",
        "    Input:  [B, D]\n",
        "    Output: [B, num_classes]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim: int, num_classes: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.w1 = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.w1.weight)\n",
        "        nn.init.zeros_(self.w1.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() != 2:\n",
        "            raise ValueError(f\"Expected 2D tensor [B, D], got shape {tuple(x.shape)}\")\n",
        "\n",
        "        B, D = x.shape\n",
        "        if D != self.embed_dim:\n",
        "            raise ValueError(f\"Expected embed_dim={self.embed_dim}, got {D}\")\n",
        "\n",
        "        return self.w1(x)"
      ],
      "metadata": {
        "id": "WIIh4yxv_SAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create classification head - MLP:"
      ],
      "metadata": {
        "id": "y6K1FYoztkSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationHeadMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Classification MLP head layer (with 1 hidden layer)\n",
        "    Input:  [B, D]\n",
        "    Output: [B, num_classes]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim: int, num_classes: int, mlp_hidden_mult: int = 4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.hidden = embed_dim * mlp_hidden_mult\n",
        "\n",
        "        self.w1 = nn.Linear(embed_dim, self.hidden)\n",
        "        self.act = nn.Tanh()\n",
        "        self.w2 = nn.Linear(self.hidden, num_classes)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.w1.weight)\n",
        "        nn.init.zeros_(self.w1.bias)\n",
        "        nn.init.xavier_uniform_(self.w2.weight)\n",
        "        nn.init.zeros_(self.w2.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() != 2:\n",
        "            raise ValueError(f\"Expected 2D tensor [B, D], got shape {tuple(x.shape)}\")\n",
        "\n",
        "        B, D = x.shape\n",
        "        if D != self.embed_dim:\n",
        "            raise ValueError(f\"Expected embed_dim={self.embed_dim}, got {D}\")\n",
        "\n",
        "        return self.w2(self.act(self.w1(x)))"
      ],
      "metadata": {
        "id": "46dIreFf7EI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine everything for our Vision Transformer model:"
      ],
      "metadata": {
        "id": "BJ5hVvrGtnh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer (ViT)\n",
        "    Paper-faithful ViT:\n",
        "      - Tokenizer: patch projection + CLS + learnable positional embeddings\n",
        "      - Stack of L TransformerEncoderBlock (Pre-LN inside each block)\n",
        "      - Final: y = LN(z_L^0)  (LayerNorm on CLS token only, Eq. 4)\n",
        "      - Classification head on top of y (linear by default; MLP optional)\n",
        "\n",
        "    Input:  [B, C, H, W]\n",
        "    Output: [B, num_classes]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_size: int = 224, patch_size: int = 16, in_chans: int = 3, num_classes: int = 1000, embed_dim: int = 192, depth: int = 6, num_heads: int = 3, mlp_hidden_mult: int = 4, attn_dropout_value: float = 0.0, proj_dropout_value: float = 0.0, mlp_dropout_value: float = 1.0, head_type: str = \"linear\"):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.head_type = head_type\n",
        "\n",
        "        self.tokenizer = ViTTokenizer(img_size, patch_size, in_chans, embed_dim)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerEncoderBlock(embed_dim, num_heads, mlp_hidden_mult, attn_dropout_value, proj_dropout_value, mlp_dropout_value)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        self.final_ln = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        if head_type == \"linear\":\n",
        "            self.head = ClassificationHeadLinear(embed_dim, num_classes)\n",
        "        elif head_type == \"mlp\":\n",
        "            self.head = ClassificationHeadMLP(embed_dim, num_classes, mlp_hidden_mult)\n",
        "        else:\n",
        "            raise ValueError(f\"Unkown head_type: {head_type!r}. Use 'linear' or 'mlp'.\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        z = self.tokenizer(x)\n",
        "        for block in self.blocks:\n",
        "            z = block(z)\n",
        "\n",
        "        cls = z[:,0,:]\n",
        "        y = self.final_ln(cls)\n",
        "\n",
        "        logits = self.head(y)\n",
        "        return logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def extract_cls(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Returns the normalized CLS representation y = LN(z_L^0) without the head.\n",
        "        \"\"\"\n",
        "        z = self.tokenizer(x)\n",
        "        for block in self.blocks:\n",
        "            z = block(z)\n",
        "        cls = z[:,0,:]\n",
        "        return self.final_ln(cls)"
      ],
      "metadata": {
        "id": "1u-BC8qf9a7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Load and Preprocessing"
      ],
      "metadata": {
        "id": "BpFyUIO8rT88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load **CIFAR-10** dataset, apply some image preprocessing on the data (using `torchvision.transforms`) and finally create a `Dataloader` object on them:"
      ],
      "metadata": {
        "id": "vA-nLYwruL7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
        "CIFAR10_STD  = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "def get_cifar10_dataloaders(batch_size = 128, num_workers = 2, img_size = 224):\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(img_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandAugment(num_ops=2, magnitude=9),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize(img_size),\n",
        "        transforms.CenterCrop(img_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
        "    ])\n",
        "\n",
        "    train_ds = torchvision.datasets.CIFAR10(root='./data/cifar10', train=True, download=True, transform=train_transform)\n",
        "    val_ds = torchvision.datasets.CIFAR10(root='./data/cifar10', train=False, download=True, transform=val_transform)\n",
        "\n",
        "    train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    val_dataloader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    return train_dataloader, val_dataloader\n",
        "\n",
        "train_dataloader, val_dataloader = get_cifar10_dataloaders()\n",
        "print(f\"Number of training batches: {len(train_dataloader)}\")\n",
        "print(f\"Number of validation batches: {len(val_dataloader)}\")"
      ],
      "metadata": {
        "id": "zqKgGz0aXW-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop and Metrics"
      ],
      "metadata": {
        "id": "QXzWp1VJr42L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an instance of the ViT model for traininig:"
      ],
      "metadata": {
        "id": "hYK8oNtmusIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "vit_model = VisionTransformer(\n",
        "    img_size=224,\n",
        "    patch_size=16,\n",
        "    in_chans=3,\n",
        "    num_classes=num_classes,\n",
        "    embed_dim=128,\n",
        "    depth=6,\n",
        "    num_heads=4,\n",
        "    mlp_hidden_mult=4,\n",
        "    attn_dropout_value=0.0,\n",
        "    proj_dropout_value=0.0,\n",
        "    mlp_dropout_value=0.1,\n",
        "    head_type=\"linear\"\n",
        ").to(device)\n",
        "\n",
        "print(f\"Total number of parameters: {sum(p.numel() for p in vit_model.parameters())}\")"
      ],
      "metadata": {
        "id": "ZBUpQ9hzfqwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up and optimizer (Adam with weight-decay) and adding `warmup` + `cosine` to ensure that the model won't explode at the beginning."
      ],
      "metadata": {
        "id": "BxpIAhx0uynz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_warnup_cosine(optimizer, warup_steps, total_steps, min_lr=0.0):\n",
        "    def lr_lambda(step):\n",
        "        if step < warup_steps:\n",
        "            return max(1e-8, float(step + 1) / float(max(1, warup_steps)))\n",
        "        progress = float(step - warup_steps) / float(max(1, total_steps - warup_steps))\n",
        "        cosine = 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "        min_factor = min_lr / max(1e-8, optimizer.param_groups[0][\"lr_initial\"])\n",
        "        return min_factor + (1.0 - min_factor) * cosine\n",
        "    return LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "epoches = 30\n",
        "base_lr = 5e-4\n",
        "weight_decay = 0.05\n",
        "warmup_epochs = 1\n",
        "\n",
        "optimizer = AdamW(vit_model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
        "# Store initial lr for scheduler math\n",
        "for pg in optimizer.param_groups:\n",
        "    pg[\"lr_initial\"] = pg[\"lr\"]\n",
        "\n",
        "total_steps = epoches * len(train_dataloader)\n",
        "warmup_steps = warmup_epochs * len(train_dataloader)\n",
        "scheduler = build_warnup_cosine(optimizer, warmup_steps, total_steps, min_lr=1e-5)"
      ],
      "metadata": {
        "id": "xwGLSgh4hDoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up accuracy metric function:"
      ],
      "metadata": {
        "id": "wYeFZel-vTkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(logits, targets, topk=(1,)):\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        _, pred = logits.topk(maxk, dim=1)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(targets.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0)\n",
        "            res.append((correct_k / targets.size(0)).item())\n",
        "        return res"
      ],
      "metadata": {
        "id": "78j7uo90pwBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define training loop (one epoch only):"
      ],
      "metadata": {
        "id": "LFv2Iqk3vXuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, data_loader, optimizer, scheduler, scaler, epoch, log_interval=50):\n",
        "    model.train()\n",
        "    t0 = time.time()\n",
        "    run_loss, run_acc = 0.0, 0.0\n",
        "\n",
        "    for step, (images, targets) in enumerate(data_loader):\n",
        "        images, targets = images.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast(enabled=torch.cuda.is_available()):\n",
        "            logits = model(images)\n",
        "            loss = F.cross_entropy(logits, targets, label_smoothing=0.1)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        loss_val = loss.item()\n",
        "        acc_val = accuracy(logits, targets, topk=(1,))[0]\n",
        "\n",
        "        run_loss += loss_val\n",
        "        run_acc += acc_val\n",
        "\n",
        "        if (step + 1) % log_interval == 0:\n",
        "            avg_loss = run_loss / (step + 1)\n",
        "            avg_acc = run_acc / (step + 1)\n",
        "            lr_now = optimizer.param_groups[0][\"lr\"]\n",
        "            print(f\"Epoch {epoch} | Step {step+1}/{len(data_loader)} | lr={lr_now:.6f} | Loss={avg_loss:.4f} | Acc={avg_acc:.4f}\")\n",
        "\n",
        "    dt = time.time() - t0\n",
        "    return run_loss / len(data_loader), run_acc / len(data_loader), dt\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    total_loss, total_acc = 0.0, 0.0\n",
        "\n",
        "    for images, targets in data_loader:\n",
        "        images, targets = images.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
        "        logits = model(images)\n",
        "        loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_acc += accuracy(logits, targets)[0]\n",
        "\n",
        "    return total_loss / len(data_loader), total_acc / len(data_loader)"
      ],
      "metadata": {
        "id": "YbtlVP82qj7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define training loop (full):"
      ],
      "metadata": {
        "id": "Vgo1TwQTvb9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_full_loop(model, train_dataloader, val_dataloader, optimizer, num_epochs):\n",
        "    scaler = GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "    train_loss_values, train_acc_values = [], []\n",
        "    val_loss_values, val_acc_values = [], []\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_loss, train_acc, train_dt = train_one_epoch(model, train_dataloader, optimizer, scheduler, scaler, epoch)\n",
        "        val_loss, val_acc = evaluate(model, val_dataloader)\n",
        "\n",
        "        best_val_acc = max(best_val_acc, val_acc)\n",
        "\n",
        "        print(f\"[Epoch {epoch}] train: loss={train_loss:.4f}, acc={train_acc:.2f}% | val: loss={val_loss:.4f}, acc={val_acc:.2f}% | time={train_dt:.1f}s\")\n",
        "\n",
        "        train_loss_values.append(train_loss)\n",
        "        train_acc_values.append(train_acc)\n",
        "        val_loss_values.append(val_loss)\n",
        "        val_acc_values.append(val_acc)\n",
        "\n",
        "    print(f\"Best val acc: {best_val_acc:.2f}%\")\n",
        "\n",
        "    history = {}\n",
        "    history[\"train_loss\"] = train_loss_values\n",
        "    history[\"train_acc\"] = train_acc_values\n",
        "    history[\"val_loss\"] = val_loss_values\n",
        "    history[\"val_acc\"] = val_acc_values\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "uWfKNGJQvm8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = train_full_loop(vit_model, train_dataloader, val_dataloader, optimizer, epoches)"
      ],
      "metadata": {
        "id": "pk9B3kXWyxtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-R0KinFZJWrk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}